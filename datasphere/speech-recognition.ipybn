{"nbformat":4,"nbformat_minor":4,"metadata":{"kernelspec":{"display_name":"Yandex DataSphere Kernel","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"notebookId":"44ad9ac3-5310-4ead-a8d2-b022f73443f9"},"cells":[{"cell_type":"markdown","source":"# Задача распознавания говорящего по голосу","metadata":{"cellId":"tll9ofc41bc7f0uchagxnm"}},{"cell_type":"markdown","source":"## I. Постановка задачи","metadata":{"cellId":"ouwakt5asjvaowvc6qahe"}},{"cell_type":"markdown","source":"На основании имеющихся данных мы хотим научиться определять говорящего. **Data-set** представляет собой набор, состоящий из\n1. Trainig corpus. Голосовые высказывания спикеров (несколько записей по каждому спикеру).\n2. Test corpus. Другие записи тех же спикеров.\n\nВсе аудиофайлы имеют длительность 10 sec и сэмплируются на частоте 16000 Hz.","metadata":{"cellId":"h33nxhqk056r5agev40cz9"}},{"cell_type":"markdown","source":"## II. Введение","metadata":{"cellId":"3qgydy9hkb5gbr1cn7b35g"}},{"cell_type":"markdown","source":"Речевой сигнал представляет собой последовательность чисел, которые определяют амплитуду говорящего. Вся концепция распознавания речи базируется на трёх основных принципах/инструментах:","metadata":{"cellId":"x86k14nyl4sqty8uqvqdof"}},{"cell_type":"markdown","source":"### 1. Framing","metadata":{"cellId":"4176ljwxii4hirtfcpy99e"}},{"cell_type":"markdown","source":"Поскольку речь не является стационарным сигналом (стационарностью называется свойство процесса сохранять свои характеристики с течением времени), её частотное содержание постоянно изменяется во времени. Чтобы выполнить хотя бы какой-нибудь анализ сигнала на коротких временных интервалах (*Short Term Fourier Transformation*), нам надо иметь возможность рассмотреть сигнал как стационарный. Чтобы достичь этой стационарности, речевой сигнал делится на ***фреймы*** длинной 20-30 ms. На такой длине можно сделать предположение о том, что форма волны не изменяется или изменяется совсем незначительно.","metadata":{"cellId":"l9jaf7129g79lfoywc22cq"}},{"cell_type":"markdown","source":"![frame-segmentation](presentation/Segmentation-of-speech-signals-frame-by-frame.png)","metadata":{"cellId":"qp8rqs3yauf5uxmkli764"}},{"cell_type":"markdown","source":"### 2. Windowing","metadata":{"cellId":"vxf2fiflkuflvc13aj2ga"}},{"cell_type":"markdown","source":"Извлечение фреймов из речевого сигнала может привести к разрывам в целых точках из-за нецелого числа периодов в извлечённом сигнале, что в свою очередь может привести к ошибочному представлению частоты (говорят, что случается *спектральная утечка* ~ *spectral leakage*). Это можно предотвратить умножением фрейма на некоторую ***оконную функцию***. Амплитуда оконной функции падает до нуля на концах фрейма, что естественным образом минимизирует амплитуду разрыва.","metadata":{"cellId":"28sl781qjhzfwxd9xvjdih"}},{"cell_type":"markdown","source":"На картинках ниже приведена иллюстррация фрейма до и после умножения на оконную функцию.","metadata":{"cellId":"ehdx8pxhuy581lbvm2tqxg"}},{"cell_type":"markdown","source":"![non-integer](presentation/noninteger1.png) ![windowing](presentation/window2.png)","metadata":{"cellId":"24o5kqlbnrpg42gxlhevrg7"}},{"cell_type":"markdown","source":"### 3. Overlapping","metadata":{"cellId":"bocap5skzajdbr5niyhxer"}},{"cell_type":"markdown","source":"Из-за windowing'а возможна ситуация, когда в результате \"сужения\" мы теряем часть информации о сигнале на концах фрейма. Чтобы нивелировать этот возможный дефект необходимо сделать ***перекрытие*** фреймов. Суть его в следующем: пусть фрейм *s* имеет длину 20-30 ms, возьмём фрейм *s+1*, который также имеет длину 20-30 ms и \"наложим\" его частично на фрейм *i*, длина перекрытия обычно равна 10-15 ms.","metadata":{"cellId":"obodxfc65fzhsfzrbyq6"}},{"cell_type":"markdown","source":"Приведём поясняющую иллюстрацию.","metadata":{"cellId":"7em8vm2qwkpi51tj08ehmh"}},{"cell_type":"markdown","source":"![overlapping](presentation/mfcc_audioframes.png)","metadata":{"cellId":"l6gvf11j5s8jxh2bp8beb"}},{"cell_type":"markdown","source":"## III. Gaussian Mixture Model (GMM)","metadata":{"cellId":"08oqlpppbhtw8s9s08t57m6"}},{"cell_type":"markdown","source":"***Gaussian Mixture Model (GMM)*** – вероятностная модель кластеризации для представления присутствия под-популяции в охватывающей популяции. Идея обучения GMM – приблизить распределение вероятности класса линейной комбинацией $k$ гауссовых распределений, которые также называются компонентами GMM.","metadata":{"cellId":"ey4mv4w0tcedw0eiugk1b"}},{"cell_type":"markdown","source":"Вероятность точек данных (векторов признака) для модели задаётся следующим образом","metadata":{"cellId":"wyliqedwfsesesr84dpk"}},{"cell_type":"markdown","source":"$\\mathbf{P} (\\mathbf{x} \\vert \\lambda) = \\sum_{k = 1}^{K} \\omega_k f_{\\mathbf{X}_k}(\\mathbf{x})$, где","metadata":{"cellId":"nupxl42vkkep6ea72eso2i"}},{"cell_type":"markdown","source":"$f_{\\mathbf{X}}(\\mathbf{x}) = \\frac{1}{(2\\pi )^{n/2} \\vert \\Sigma \\vert^{1/2}} e^{-\\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu})^{\\top} \\Sigma^{-1} (\\mathbf{x} - \\mathbf{\\mu})},\\; \\mathbf{x} \\in \\mathbb{R}^n$, \n\nгде $\\vert \\Sigma\\vert$ — определитель матрицы $\\Sigma$, а $\\Sigma^{-1}$ — матрица, обратная к $\\Sigma$.\n","metadata":{"cellId":"30v9zelnbql38624of9mz"}},{"cell_type":"markdown","source":"Training data $X_i$ класса $\\lambda$ используется для оценки значений всех параметров.","metadata":{"cellId":"xkudscbocf44a4q5fubmr"}},{"cell_type":"markdown","source":"Вначале определяется $k$ классв в данных $K$-средним алгоритмом и с весами $\\omega = 1/k$ для каждого кластера. Затем $k$ гауссовых распределений фитят $k$ кластеров, все параметры при этом обновляются итеративно.","metadata":{"cellId":"lwcxro34pr4vekucnlf7g"}},{"cell_type":"markdown","source":"## IV. Демонстрация решения","metadata":{"cellId":"onc2mbh3vuy9wadibl5r"}},{"cell_type":"markdown","source":"### Установка и импорт необходимых пакетов","metadata":{"cellId":"435wj1zq66s4umyt6o5se3"}},{"cell_type":"code","source":"# После установки потребуется restart kernal\n%pip install numba==0.48.0\n%pip install librosa\n%pip install cffi==1.14.2\n%pip show numba","metadata":{"cellId":"tz8jup8rqxf00gno8y6tg","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nimport os\nfrom tqdm import tqdm\n\nimport numpy as np\nimport pandas as pd\nimport scipy\n\nimport librosa\nfrom librosa import display\nfrom IPython.display import Audio \n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import accuracy_score\nimport pickle","metadata":{"cellId":"hxe1msiec5fztvibbp8y8f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Получение данных","metadata":{"cellId":"ajci27r87hczre6vmwhoim"}},{"cell_type":"markdown","source":"Snippets -> Extract Zip file.py","metadata":{"cellId":"376zvspk3tqx3z6ne5pc"}},{"cell_type":"markdown","source":"### Генерация аудио-признаков","metadata":{"cellId":"58sjsz6c7u8wpm7arf1mia"}},{"cell_type":"markdown","source":"Создадим функцию, которая будет извлекать аудио-признаки","metadata":{"cellId":"lpwude9n8od1c61dun5cu9"}},{"cell_type":"code","source":"def audio_features_extraction(sample, sample_rate, n_fft):\n    \n    \"\"\" \n        sample - audio time series\n        sample_rate - sampling rate of sample\n        n_fft = frame size\n    \"\"\"\n    \n    # librosa.feature.mfcc - вычисляет коэффициенты MFCCs.\n    # MFCCs трансформируют значение сигнала в кепстр – один из видов гомоморфной обработки сигналов, \n    # функция обратного преобразования Фурье от логарифма спектра мощности сигнала. \n    # Основная задача: охарактеризовать фильтр и отделить исходную часть\n    # (на примере с голосом человека – охарактеризовать вокальный тракт).\n    mfcc = librosa.feature.mfcc(y=sample, \n                                n_fft=n_fft, # размер фрейма\n                                window='hann',  # оконная функция (windowing)\n                                hop_length=int(n_fft*0.5), # размер перекрытия фреймов (overlapping)\n                                sr=sample_rate, \n                                n_mfcc=20)\n    features = np.mean(mfcc, axis=1)\n    \n    # librosa.feature.zero_crossing находит нулевые переходы для сигнала.\n    zero_crossings = sum(librosa.zero_crossings(sample, pad=False))\n    features = np.append(zero_crossings, features)\n    \n    # librosa.feature.spectral_centroid вычисляет спектральный центроид.\n    # Каждый фрейм амплитудной спектрограммы нормализуется и обрабатывается как распределение по частотным элементам,\n    # из которого извлекается среднее значение (центроид) для каждого фрейма.\n    spec_cent = librosa.feature.spectral_centroid(y=sample,n_fft=n_fft, hop_length=int(n_fft*0.5), window='hann', sr=sample_rate).mean()\n    features = np.append(spec_cent, features)\n    \n    # librosa.feature.spectral_flatness вычисляет cпектральную плоскостность.\n    # Спектральная плоскостность - количественная мера того, насколько звук похож на шум, а не на тон.\n    spec_flat = librosa.feature.spectral_flatness(y=sample,n_fft=n_fft, hop_length=int(n_fft*0.5), window='hann').mean()\n    features = np.append(spec_flat, features)\n    \n    # librosa.feature.spectral_bandwith вычисляет спектральную полосу пропускания p-ого порядка.\n    spec_bw = librosa.feature.spectral_bandwidth(y=sample,n_fft=n_fft, hop_length=int(n_fft*0.5), window='hann', sr=sample_rate).mean()\n    features = np.append(spec_bw, features)\n    \n    # librosa.feature.spectral_rolloff вычисляет roll-off частоту для каждого фрейма.\n    # Roll-off частота определяется как центральная частота для интервала спектрограммы.\n    rolloff = librosa.feature.spectral_rolloff(y=sample, n_fft=n_fft, hop_length=int(n_fft*0.5), window='hann', sr=sample_rate).mean()\n    features = np.append(rolloff, features)\n    \n    return features","metadata":{"cellId":"ay7n7dmrl1tbi4y68osks4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Читаем последовательно аудио-файлы и извлекаем аудио-признаки","metadata":{"cellId":"u4awkxs4b43ixoavmci03"}},{"cell_type":"code","source":"start = time.time()\n\nsource   = \"./data/development_set/\"  \ndest = \"./data/speaker-models/\"\ntrain_file = \"./data/development_set_enroll.txt\"\nfile_paths = open(train_file, 'r')\n \nn_fft = 1024\n\n# Подготовка датасетов для обучения моделей.\nfeatures = pd.DataFrame()\nspeakers = pd.DataFrame()\n\n# Последовательное чтение аудио-файлов из тренировочного семпла.\nfor path in tqdm(file_paths, desc='Features extractions '):\n    path = path.replace(\"\\\\\",\"/\").strip()\n    speaker = path.split(\"-\",1)[0]\n\n    # librosa.load - загрузка аудио-файла.\n    sample, sample_rate = librosa.load(source+path)\n    \n    # Извлечение аудио-признаков.\n    features = features.append(pd.Series(audio_features_extraction(sample, sample_rate, n_fft)), ignore_index=True) \n    speakers = speakers.append({'speaker' : speaker}, ignore_index=True)\n \nprint('Execution time: ', round((time.time() - start),2))   ","metadata":{"cellId":"yoicsxqstrd9xvvpxg7ajk","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Нормируем признаки","metadata":{"cellId":"tgq0it0wvjc18c8hqbwwwa"}},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(features)\nfeatures_scaled = pd.DataFrame(scaler.transform(features))\npickle.dump(scaler, open('scaler','wb'))","metadata":{"cellId":"s9yseie4829a8fnuy3fnf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Обучение моделей","metadata":{"cellId":"5sgff9ktpfw11k3nx5s2l"}},{"cell_type":"markdown","source":"Обучаем модели и сохраняем в директорию `speaker-models/`","metadata":{"cellId":"ip5mke42jaz2vtjtvpbs"}},{"cell_type":"code","source":"start = time.time()\n\nfeatures_smpl = pd.DataFrame()\ncount = 1\n\nif not os.path.exists(os.path.dirname(dest)):\n    try:\n        os.makedirs(os.path.dirname(dest))\n    except: \n        pass\n\nfor i in range(speakers.shape[0]):\n    speaker = speakers.iloc[i:i+1].values[0]\n    if count == 1:\n        speaker_prev = speaker     \n    else :\n        # Обучение модели GaussianMixture для каждого спикера\n        if (speaker_prev != speaker) | (i == len(speakers)-1) :\n            gmm = GaussianMixture(n_components = min(16, features_smpl.shape[0]), \n                                  max_iter = 200, covariance_type='diag', n_init = 3)\n            gmm.fit(features_smpl)\n            \n            # Сохранение полученной модели в pickle\n            pickle.dump(gmm, open((dest+speaker_prev)[0],'wb'))\n            features_smpl = pd.DataFrame()\n            count = 0\n            speaker_prev = speaker\n            \n    # Сбор данных по одному спикеру.\n    features_smpl  = features_smpl.append(features_scaled.iloc[i:i+1], ignore_index=True)\n    count = count+1\n\nprint('Execution time: ', round((time.time() - start),2)) ","metadata":{"cellId":"scidkmetempbwvwfhxfr","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Тестирование на тестовом семпле","metadata":{"cellId":"pdej0sspb28cctnvi2nmcv"}},{"cell_type":"code","source":"start = time.time()\n\nsource   = \"./data/development_set/\"  \ndest = \"./data/speaker-models/\"\ntest_file = \"./data/development_set_test.txt\"\nfile_paths = open(test_file,'r')\n \nn_fft = 1024\n\nfeatures = pd.DataFrame()\nresult = pd.DataFrame()\n\ngmm_files = [os.path.join(dest,fname) for fname in os.listdir(dest)]\ngmm_files = [fname for fname in gmm_files if not '.ipynb' in fname]\nmodels    = [pickle.load(open(fname,'rb')) for fname in gmm_files ]\nscaler    = pickle.load(open('scaler','rb'))\n\n# Последовательное чтение аудио-файлов из тестового семпла.\nfor path in tqdm(file_paths, desc='Score test sample '):\n    \n    features = pd.DataFrame()\n    \n    path = path.replace(\"\\\\\",\"/\").strip()\n    speaker = path.split(\"-\",1)[0]\n    \n    # librosa.load - загрузка аудио-файла.\n    sample, sample_rate = librosa.load(source+path)\n    \n    # Извлечение аудио-признаков.\n    features = features.append(pd.Series(audio_features_extraction(sample, sample_rate, n_fft)), ignore_index=True) \n\n    # Нормирование аудио-признаков.\n    features = scaler.transform(features)\n    \n    # Скоринг каждой моделью\n    log_likelihood = np.zeros(len(models)) \n    for i in range(len(models)):\n        gmm = models[i] \n        scores = np.array(gmm.score(features))\n        log_likelihood[i] = scores.sum()\n        \n    # Выбор спикера по наибольшему скору\n    winner = np.argmax(log_likelihood)\n    result = result.append({'speaker' : speaker,\n                           'winner' : gmm_files[winner].split(\"/\",3)[-1]}, ignore_index=True)\n    \nprint('Execution time: ', round((time.time() - start),2))   \nprint('Точность угадывания, %: ', round(100*result[result['speaker']==result['winner']].shape[0]/result.shape[0],2))\n# Во время работы ячейки могут появляться предупреждения о невозможности сериализации переменных\n# Все переменные, что указаны в этих предупреждениях будут доступны только на той конфигурации ячеек, на которой была запущенна данная","metadata":{"cellId":"fj8md7md185qnnvgxe5pqo","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"cellId":"4muiau01otcezzawzfqztt"},"outputs":[],"execution_count":null}]}