{"nbformat":4,"nbformat_minor":4,"metadata":{"kernelspec":{"display_name":"Yandex DataSphere Kernel","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"notebookId":"19649fa2-f6d9-447e-a71e-e50e9f01e27a"},"cells":[{"cell_type":"markdown","source":"# Обучение языковых моделей для STT\n\n## Пошаговый гайд\n\n1. Подготовьте данные, необходимые для обучения языковой модели. Для этого требуется подготовить тексты, а также наборы из размеченных валидационных и тестовых аудиозаписей\n \n  * Набор текстов, необходимый для обучения языковой модели, должен представлять из себя таблицу в формате TSV, состоящую из двух столбцов, где второй столбец соответствует самим текстам (предложениям), а первый столбец &mdash; количеству вхождений этих текстов в исходный датасет. Также к текстам разметки предъявляются следующие требования:\n \n    * все тексты должны быть приведены к нижнему регистру\n    \n    * тексты не должны содержать символов, отличных от символов русского алфавита и пробелов\n    \n    * тексты не должны содержать букв \"ё\", все такие буквы следует заменять на \"е\"\n    \n    * тексты не должны содержать пробелов в начале и в конце, также все слова должны быть разделены одним пробелом\n    \n    Следует отметить, что аналогичные требования также предъявляются и к разметке аудиозаписей\n \n  * Валидационные аудиозаписи и их разметка должны быть сохранены в отдельной папке. Эти данные будут использоваться для подбора гиперпараметров обучаемой модели.\n \n    Все аудиозаписи должны иметь следующий формат:\n    \n    * одноканальное аудио в формате WAV\n    \n    * sample rate: 8000, 16000 или 48000\n    \n    * разрядность: 16 бит, little endian\n \n    Кроме того, вместе с валидационными аудиозаписями должен храниться файл `records.json` с разметкой аудио. Этот файл должен иметь следующий формат:\n    \n    ```\n    {\n        \"<audio1_name>.wav\": \"разметка 1-ой записи\",\n        ...\n        \"<audioN_name>.wav\": \"разметка N-ой записи\",\n    }\n    ```\n    &nbsp;\n  * Тестовые аудиозаписи и их разметка должны быть подготовлены аналогично набору валидационных записей. Эти данные не будут использоваться при обучении, но по ним будет получены распознавания с помощью обученной языковой модели, которые затем могут быть использованы для оценки её качества","metadata":{"cellId":"78omt64naxwj6e98h66w5n"}},{"cell_type":"markdown","source":"2. Запустите команду\n   \n  ```\n    > #!nirvana\n    > sk_train_language_model --train-texts PATH --validation-dir PATH --test-dir PATH --model PATH --recognitions PATH\n    >\n  ```\n  * `--train-texts PATH` &mdash; путь до TSV-таблицы с текстами\n  \n  * `--validation-dir PATH` &mdash; путь до директории с валидационными аудиозаписями\n  \n  * `--test-dir PATH` &mdash; путь до директории с тестовыми аудиозаписями\n  \n  * `--model PATH` &mdash; путь, по которому будет сохранена полученная языковая модель\n  \n  * `--recognitions PATH` &mdash; путь, по которому будет сохранены распознавания аудиозаписей из тестового набора (в формате JSON)","metadata":{"cellId":"812jsg83gv958gmu2zg2j5"}},{"cell_type":"markdown","source":"3. Дождитесь завершения команды. На этом этапе вы также можете закрыть вкладку с ноутбуком и вернуться за результатам позже. Обучение модели отработает в фоновом режиме.\n\n   Время обучения напрямую зависит от количества предоставленных текстов, а также от количества аудиозаписей. \n   \n   Количество данных, требуемое для обучения хорошей языковой модели, может сильно варьироваться в зависимости от конкретной задачи. Тем не менее, _рекомендуется_ использовать не менее 1МБ текстов, а также от 100 до 2000 валидационных и тестовых аудиозаписей.","metadata":{"cellId":"i3fnc2187amw38rmpebim9"}},{"cell_type":"markdown","source":"## Подготовка данных\n\nПриведём пример того, как подготовить данные для обучения кастомной языковой модели.\n\nПредположим, что изначально в нашем распоряжении имеется набор текстов `raw_data/dataset.txt`, набор аудиозаписей, хранящийся в папке `raw_data/audio`, а также разметка этих аудиозаписей, хранящаяся в папке `raw_data/references`.\n\n### Подготовка текстов\n\nВ первую очередь подготовим тексты для обучения языковой модели. Для этого разобьём тексты нашего датасета на предложения, нормализуем их и сохраним в файл `prepared_data/texts.tsv` в соответствии с описанным выше форматом. \n\nНиже приведён возможный пример такой обработки текстов:","metadata":{"cellId":"xcdl2yawqsyt76h7v18rq"}},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\n\nimport re\nfrom pathlib import Path\nfrom nltk.tokenize import sent_tokenize\nfrom collections import defaultdict\n\n\ndef normalize_sentence(text: str) -> str:\n    # приводим текст предложения к нижнему регистру, заменяем буквы \"ё\" на \"е\"\n    text = text.lower().replace('ё', 'е')\n    \n    # удаляем из текста спец. символы, оставляем только русскоязычные символы и пробелы\n    text = re.sub(r'[^а-я ]', ' ', text)\n    \n    # также удаляем все лишние пробелы\n    text = ' '.join(filter(len, text.split()))\n    \n    return text\n\n\ndef prepare_texts(src_path: Path, dst_path: Path, merge_duplicates: bool = True):\n    sentences = []\n    \n    text = src_path.read_text()\n    \n    # разбиваем текст на предложения и нормализуем их\n    for paragraph in text.splitlines():\n        for sentence in sent_tokenize(paragraph):\n            sentences.append(normalize_sentence(sentence))\n\n    if not dst_path.parent.exists():\n        dst_path.parent.mkdir(parents=True)\n        \n    with dst_path.open('w') as f:\n        if not merge_duplicates:\n            # в простом варианте сохраняем все предложения отдельно, допуская дубликаты\n            for sentence in sentences:\n                f.write(f'1\\t{sentence}\\n')\n        else:\n            # опционально можем избавиться от дубликатов, сохранив с каждым предложением количество его вхождений в текст\n            sentence_count = defaultdict(int)\n            for sentence in sentences:\n                sentence_count[sentence] += 1\n            for sentence, count in sentence_count.items():\n                f.write(f'{count}\\t{sentence}\\n')","metadata":{"cellId":"roax91mx559j2vf76byqvq","trusted":true},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"prepare_texts(\n    src_path=Path('examples', 'speechkitpro', 'fit_lm', 'raw_data', 'dataset.txt'), \n    dst_path=Path('examples', 'speechkitpro', 'fit_lm', 'prepared_data', 'texts.tsv')\n)","metadata":{"cellId":"5msf6puc3cenctsa2kg58f","trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### Подготовка аудио\n\nДалее подготовим аудиозаписи. \n\nДля этого разобъём весь набор аудиозаписей на валидационный и тестовый наборы, а затем переведём аудиозаписи и их разметку в требуемый нам формат, разместив их в папках `prepared_data/val_audio` и `prepared_data/test_audio` соответственно.","metadata":{"cellId":"anulmrh9nir3v4jha06xmm"}},{"cell_type":"code","source":"%pip install pydub\n\nimport random\nimport json\nfrom pydub import AudioSegment\nfrom typing import List\n\n\ndef prepare_audio_set(src_audio_dir: Path, reference_paths: List[Path], dst_dir: Path):\n    if not dst_dir.exists():\n        dst_dir.mkdir(parents=True)\n    \n    records_desc = {}\n    \n    for reference_path in reference_paths:\n        # по имени файла с разметкой получаем путь до соответствующей аудиозаписи\n        audio_name = reference_path.name.replace('.txt', '.wav')\n        \n        src_audio_path = src_audio_dir / audio_name\n        dst_audio_path = dst_dir / audio_name\n        \n        with src_audio_path.open('rb') as f:\n            audio = AudioSegment.from_wav(f)\n            \n        # приводим все аудиозаписи к единому формату, с которым работают наши инструменты\n        audio = audio.set_channels(1).set_sample_width(2).set_frame_rate(16000)\n        \n        # и сохраняем в формате WAV\n        with dst_audio_path.open('wb') as f:\n            audio.export(out_f=f, format='wav')\n        \n        # кроме того, нормализуем текст разметки\n        records_desc[audio_name] = normalize_sentence(reference_path.read_text())\n    \n    with (dst_dir / 'records.json').open('w') as f:\n        json.dump(records_desc, f, indent=2)\n\n\n# здесь получим список всех записей (их разметок) и поровну разделяем его на валидационный и тестовый наборы\ndef prepare_audio(src_audio_dir: Path, src_reference_dir: Path, dst_dir: Path):\n    random.seed(0)\n    \n    reference_paths = list(filter(lambda f: f.name.endswith('.txt'), src_reference_dir.iterdir()))\n    random.shuffle(reference_paths)\n    \n    split_size = len(reference_paths) // 2\n    \n    prepare_audio_set(src_audio_dir, reference_paths[:split_size], dst_dir / 'val_audio')\n    prepare_audio_set(src_audio_dir, reference_paths[split_size:], dst_dir / 'test_audio')","metadata":{"cellId":"cnxmza5cmrnrlqxv84cjd","trusted":true},"outputs":[{"name":"stdout","text":"Collecting pydub\n  Downloading pydub-0.24.1-py2.py3-none-any.whl (30 kB)\nInstalling collected packages: pydub\nSuccessfully installed pydub-0.24.1\n\u001B[33mWARNING: Target directory /home/jupyter/work/pyenv/pydub already exists. Specify --upgrade to force replacement.\u001B[0m\n\u001B[33mWARNING: Target directory /home/jupyter/work/pyenv/pydub-0.24.1.dist-info already exists. Specify --upgrade to force replacement.\u001B[0m\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"prepare_audio(\n    Path('examples', 'speechkitpro', 'fit_lm', 'raw_data', 'audio'), \n    Path('examples', 'speechkitpro', 'fit_lm', 'raw_data', 'references'), \n    Path('examples', 'speechkitpro', 'fit_lm', 'prepared_data')\n)","metadata":{"cellId":"9a0raeht69f9u9i4rfmhke","trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Обучение модели\n\nТеперь мы можем вызвать команду `sk_train_language_model`, указав в качестве входных параметров пути до текстов и наборов аудиозаписей.\n\nПо завершению исполнения ячейки в файл `lm` будет сохранена языковая модель, а в файл `recognitions.json` будет сохранены результаты потокового распознавания записей из тестового набора в зависимости от некоторых настроек распознавания.","metadata":{"cellId":"57xupfyjmfhrrlo7n7v8y7"}},{"cell_type":"code","source":"#!nirvana\nsk_train_language_model --train-texts ./examples/speechkitpro/fit_lm/prepared_data/texts.tsv --validation-dir ./examples/speechkitpro/fit_lm/prepared_data/val_audio --test-dir ./examples/speechkitpro/fit_lm/prepared_data/test_audio --model ./examples/speechkitpro/fit_lm/lm --recognitions ./examples/speechkitpro/fit_lm/recognitions.json","metadata":{"cellId":"906235urfja1w7jkni7ts","trusted":true},"outputs":[{"name":"stdout","text":"\u001B[1m[Step 1/2]\u001B[0m Preparing input data...\nPreparing training texts from \u001B[1m./examples/speechkitpro/fit_lm/prepared_data/texts.tsv\u001B[0m\nPreparing validation audio files from \u001B[1m./examples/speechkitpro/fit_lm/prepared_data/val_audio\u001B[0m\nPreparing test audio files from \u001B[1m./examples/speechkitpro/fit_lm/prepared_data/test_audio\u001B[0m\n\n\u001B[1m[Step 2/2]\u001B[0m Running training process...\n\n 100.0% [==================================================>]\n\nTraining process has finished successfully.\nLanguage model is saved to \u001B[1m./examples/speechkitpro/fit_lm/lm\u001B[0m\nResults calculated by this model and test data set are saved to \u001B[1m./examples/speechkitpro/fit_lm/recognitions.json\u001B[0m\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Оценка качества модели\n\nНаконец, мы можем оценить качество полученной модели на тестовой выборке, чтобы затем иметь возможность выбрать лучшую модель.\n\nПодробнее о том, как \"правильно\" оценить качество модели, рассказано [тут](?). Здесь же мы приведём самый простой вариант оценки качества модели в зависимости от использованных параметров распознавания с помощью метрики WER.","metadata":{"cellId":"lob1u0qv2yovas0d5lyu3"}},{"cell_type":"code","source":"import json\nfrom pathlib import Path\n\nfrom stt_metrics import evaluate_wer\n\nreferences = json.loads(Path('examples', 'speechkitpro', 'fit_lm', 'prepared_data', 'test_audio', 'records.json').read_text())\nrecognition_sets = json.loads(Path('examples', 'speechkitpro', 'fit_lm', 'recognitions.json').read_text())\n\nfor recognition_set in recognition_sets:\n    mean_wer, full_report = evaluate_wer(references=references, hypotheses=recognition_set['recognitions'])\n    print(f'Recognition params: {recognition_set[\"params\"]}')\n    print(f'Mean WER: {mean_wer}\\n')","metadata":{"cellId":"txlpmi7cil4segayo9w2b","trusted":true},"outputs":[{"name":"stdout","text":"Recognition params: {'noise_reduction': 'Heavy'}\nMean WER: 0.35714285714285715\n\nRecognition params: {'noise_reduction': 'Normal'}\nMean WER: 0.21885521885521886\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"cellId":"kbb944hdlpkij3cj3syc"},"outputs":[],"execution_count":null}]}